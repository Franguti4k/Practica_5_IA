# ğŸ§ª Regression with a Wild Blueberry Yield Dataset â€” Kaggle Competition

## ğŸ“˜ DescripciÃ³n del Notebook

Este notebook detalla el proceso completo seguido para abordar la competiciÃ³n de Kaggle, desde una pequeÃ±a exploraciÃ³n inicial de datos hasta el diseÃ±o del modelo final. Se hace especial Ã©nfasis en tÃ©cnicas de optimizaciÃ³n de modelos mediante **Optuna** y la creaciÃ³n de un **ensemble ajustado de CatBoost y LightGBM**.

---

## ğŸ Sobre la CompeticiÃ³n

**Objetivo:**  
Predecir la produccion (`yield`) de de arÃ¡ndanos silvestres basados en datos del dataset.

**MÃ©trica de evaluaciÃ³n:**  
> `Mean Absolute Error (MAE)` sobre un conjunto de test privado (80% del total oculto por Kaggle hasta el cierre).

**Estrategia clave:**  
> Optimizamos hiperparÃ¡metros con `Optuna`, y combinamos los mejores modelos con un ensemble ponderado para balancear rendimiento y robustez.

---

## ğŸ† Resultado Final

- ğŸ“Š **Leaderboard privado:** `MAE = 241.409314`  
- ğŸ¥‰ **PosiciÃ³n final:** **Top 3** de la clasificaciÃ³n general ğŸ‰  

---

### ğŸ”— TecnologÃ­as usadas

- Python (Pandas, Numpy)
- LightGBM y CatBoost
- Optuna
- Scikit-learn
- matplotlib
- seaborn
